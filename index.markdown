---
permalink: /
author_profile: true
---

### About Me

I am a Ph.D. student at KAIST. My research interests include:

- Deep Learning Optimization
- Efficient ML
- Preference Optimization

---

### Education

- **Ph.D.** in Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST) \\
*Mar. 2020 ~ Feb. 2025*
- **M.S.** in School of Computing, Korea Advanced Institute of Science and Technology (KAIST) \\
*Mar. 2018 ~ Feb. 2020*
- **B.S.** in Mathematical Science & School of Computing, Korea Advanced Institute of Science and Technology (KAIST) \\ 
*Feb. 2012 ~ Feb. 2018*

---

### Working Papers

- **Riemannian Stochastic Proximal Methods Meet Adaptive Gradients over Matrix Manifolds** \\
<!-- <a href="#">[paper]</a> \\ -->
<u>Jihun Yun</u>, Aurélie C. Lozano, Eunho Yang

- **Revised NTK Analysis of Optimization and Generalization with Its Extensions to Arbitrary Initialization** \\
<!-- <a href="#">[paper]</a> \\ -->
<u>Jihun Yun</u>, Kyungsu Kim, Eunho Yang

- **Elucidating Subspace Perturbation in Zeroth-Order Optimization: Theory and Practice at Scale**


### Publications

- **Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model** \\
<a href="https://arxiv.org/pdf/2506.01523">[paper]</a> \\
<u>Jihun Yun</u>\*, Juno Kim\*, Jongho Park, Junhyuck Kim, Jongha Jon Ryu, Jaewoong Cho, Kwang-Sung Jun (*: equal contribution) \\
<span style="color:darkred">**ICML 2025 MoFA Workshop**</span> 2025

- **LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding** \\
<a href="https://openreview.net/pdf?id=98d7DLMGdt">[paper]</a> \\
Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, <u>Jihun Yun</u>, Souvik Kundu, Sung-Yub Kim, Eunho Yang \\
<span style="color:darkred">**ICLR**</span> 2025

- **TEDDY: Trimming Edges with Degree-based Discrimination StrategY** \\
<a href="https://openreview.net/pdf?id=5RUf9nEdyC">[paper]</a> \\
Hyunjin Seo\*, <u>Jihun Yun</u>\*, Eunho Yang (*: equal contribution) \\
<span style="color:darkred">**ICLR**</span> 2024

- **Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds** \\
<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/cf701db0e3b4d0b8681ca6915ac3e87e-Paper-Conference.pdf">[paper]</a> \\
<u>Jihun Yun</u>, Eunho Yang \\
<span style="color:darkred">**NeurIPS**</span> 2023

- **AdaBlock: SGD with Practical Block Diagonal Matrix Adaptation for Deep Learning** \\
<a href="https://proceedings.mlr.press/v151/yun22a/yun22a.pdf">[paper]</a> \\
<u>Jihun Yun</u>, Aurélie C. Lozano, Eunho Yang \\
<span style="color:darkred">**AISTATS**</span> 2022

- **Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss** \\
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Cluster-Promoting_Quantization_With_Bit-Drop_for_Minimizing_Network_Quantization_Loss_ICCV_2021_paper.pdf">[paper]</a> \\
Jung Hyun Lee\*, <u>Jihun Yun</u>\*, Sung Ju Hwang, Eunho Yang (*: equal contribution) \\
<span style="color:darkred">**ICCV**</span> 2021

- **Adaptive Proximal Gradient Methods for Structured Neural Networks** \\
<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/cc3f5463bc4d26bc38eadc8bcffbc654-Paper.pdf">[paper]</a> \\
<u>Jihun Yun</u>, Aurélie C. Lozano, Eunho Yang \\
<span style="color:darkred">**NeurIPS**</span> 2021

- **Trimming the $\ell_1$ Regularizer: Statistical Analysis, Optimization, and Applications to Deep Learning** \\
<a href="https://proceedings.mlr.press/v97/yun19a/yun19a.pdf">[paper]</a> \\
<u>Jihun Yun</u>, Peng Zheng, Eunho Yang, Aurélie C. Lozano, Aleksandr Aravkin \\
<span style="color:darkred">**ICML**</span> 2019 (Oral Presentation, 4.64%)


---

### Invited Talks

<!-- 발표가 있으면 아래 형식으로 추가하세요 -->
<!-- - **발표 제목**, 장소/행사명, 날짜 -->
